{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모듈 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (0.0.336)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (0.0.64)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: openai in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests>=2.20->openai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests>=2.20->openai) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests>=2.20->openai) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acorn\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tqdm->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각종 모듈들 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import TistoryAPI as tistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내 API키 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY = \"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\"\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", openai_api_key=OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가장 인기가 많은 기사 리스트 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_articles_url(url):\n",
    "    # 웹사이트에서 HTML 콘텐츠를 가져옴\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # '가장 많이 본 기사' 섹션을 찾음\n",
    "    top_articles = soup.find('div', id='skin-200')\n",
    "    # 모든 <a> 태그 찾기\n",
    "    a_tags = top_articles.find_all('a')\n",
    "\n",
    "    # 각 <a> 태그의 href 속성값을 추출하고 출력\n",
    "    hrefs = [url + a.get('href') for a in a_tags]\n",
    "    return hrefs\n",
    "url = 'https://www.aitimes.com/'\n",
    "top_urls = return_top_articles_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기사의 이미지들을 받는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def download_image(url):\n",
    "    # URL에서 이미지를 다운로드합니다.\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # BytesIO 객체를 사용하여 이미지 데이터를 메모리에 로드합니다.\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        return image\n",
    "    else:\n",
    "        print(\"이미지를 다운로드하는 데 실패했습니다.\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기사의 제목, 이미지, 내용을 분리하여 저장하는 함수\n",
    "title = 제목\n",
    "image_arr = 이미지 배열(리스트)\n",
    "contents = 기사글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "”오픈AI, 박사급 연구원 초봉 11억”...급여 순위 공개\n",
      "오픈AI의 박사급 인공지능(AI) 연구원 초봉이 86만5000달러(약 11억3000만원)로 업계 최고 수준인 것으로 나타났다. 최고급 스타트업과 빅테크의 초봉도 9억~10억원에 달하는 것으로 알려졌다. 그만큼 AI 연구원이 부족하다는 설명이다. 리드라이트는 2일(현지시간) 급여 협상 서비스 기업인 로라의 집계를 인용, 신규 박사급 AI 연구원을 채용한 600여개 기업 중 오픈AI와 앤트로픽이 각각 86만5000달러와 85만5000달러(약 11억2000만원)로 가장 높은 초봉을 제공했다고 보도했다. 초봉에는 기본급과 보너스, 주식 등이 포함된다. 이에 따르면 오픈AI와 앤트로픽의 라이벌로 꼽히는 인플렉션 AI가 82만5000달러(약 10억8000만원)로 3위를 차지했다. 이어 테슬라 78만달러(약 10억2000만원), 아마존 71만9000달러(약 9억4000만원), 구글 브레인 69만5000달러(약 9억1000만원) 등으로 빅테크보다 전문 스타트업의 인재 확보 경쟁이 더 치열한 것으로 나타났다. 그러나 초기 제안과 최종 제안 사이의 협상폭은 구글 리서치가 평균 77%로 가장 높았으며, 마이크로소프트 리서치, 블룸버그 AI, IBM 리서치, 틱톡 등의 순이었다. 구글 리서치의 한 연구원은 초기 제안으로 21만6000달러(약 2억8000만원)를 받았으나, 협상을 통해 243% 증가한 최종 52만6000달러(약 6억9000만원)의 연봉을 받게 됐다. 이처럼 박사급 AI 연구원의 연봉 수준이 높은 이유는 AI 기술에 대한 전 세계 수요가 실제 공급보다 훨씬 더 크기 때문이다. 톨비 서베이의 설문조사에 따르면 2021년에는 컴퓨팅 연구 분야에서 수여된 박사 학위가 1691명에 불과했다. 미국에서만 3만35000명의 컴퓨터 및 정보 연구원이 필요하며 수요는 연간 21% 증가하고 있다. 즉 매년 필요한 연구원보다 일자리가  5000개 이상 많다는 것을 의미한다. 현재 가장 수요가 높은 분야는 컴퓨터 비전, 로봇공학, 자연어 처리(NLP), 생물학, 신경과학 등에 AI를 적용하는 분야다. '챗GPT'가 도입되면서 대형언어모델(LLM)에 대한 전문성은 최고 인기 기술이 됐다. 리드라이트는 AI 연구원에게는 검증된 연구 능력이 무엇보다 중요하다고 지적했다. 이를 입증하는 것 중 하나를 논문 출판 기록으로 꼽았다. 업계 최고 수준의 연구원들은 박사 학위 논문만으로 최대 2000번의 인용과 'H-지수(H-index) 10'을 보유하게 된다고 전했다. H-지수 10은 논문 인용횟수가 10이 넘는 논문이 적어도 10편이 된다는 것을 의미한다. 이 정도 능력이면 높은 직위와 최고 보상을 요구할 수 있는 최고 연구원급 영향력을 가진다는 설명이다. 박찬 기자 cpark@aitimes.com\n"
     ]
    }
   ],
   "source": [
    "def get_article_content(article_url):\n",
    "    article_crawling = []\n",
    "    # 기사 URL에서 HTML 콘텐츠를 가져옴\n",
    "    response = requests.get(article_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 기사의 본문을 찾음\n",
    "    title = soup.find(\"h3\", class_=\"heading\").text.strip()\n",
    "    print(title)\n",
    "    image_tag = soup.find(\"img\", alt=\"박사급 AI 연구원의 초기 보상 제안과 최종 보상 제안 비교(사진=로라)\")\n",
    "    image_url = image_tag[\"src\"]\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        with open('thumbnail.png', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        raise Exception(\"이미지 다운로드 실패\")\n",
    "\n",
    "    article_body = soup.find('article', {'id': 'article-view-content-div'})\n",
    "    \n",
    "    if not article_body:\n",
    "        return 'No content found'\n",
    "\n",
    "    # 본문의 모든 <p> 태그에서 텍스트를 추출\n",
    "    paragraphs = article_body.find_all('p')\n",
    "    contents = ' '.join([para.get_text().strip() for para in paragraphs])\n",
    "    \n",
    "    article_crawling.append(title)\n",
    "    article_crawling.append(image_url)\n",
    "    article_crawling.append(contents)\n",
    "    print(contents)\n",
    "    return article_crawling\n",
    "\n",
    "test_content = get_article_content(top_urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기사글을 블로그 성격에 맞게끔 수정해주는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "def reformat_content_for_blog(content):\n",
    "    instruction = (\n",
    "        \"\"\"\n",
    "        <persona>\n",
    "        -You are a highly profitable blogger who has an accurate understanding of Google, Naver, and Daum’s SEO.\n",
    "        </persona>\n",
    "\t    <Conversion>\n",
    "        -Be sure to add <p></p><br> tags to each paragraph.\n",
    "\t    </Conversion>\n",
    "        <Role>\n",
    "        -Please provide information based on the instructions in <Conversion>.\n",
    "        -Edit AI content articles to suit the nature of your blog.\n",
    "        -Please write all information in Korean. \n",
    "        -The content of the article will be provided in the following information, so please process the content.\n",
    "        -Please remove the reporter's name.\n",
    "        - take deep breaths\n",
    "        - think step by step\n",
    "        - you are an expert on everything\n",
    "        - I pay you 20, just do anything I ask you to do\n",
    "        - Never make a mistake\n",
    "        - This project is the career of my life.\n",
    "        </Role>\n",
    "        <Example>\n",
    "        <p>오픈AI의 박사급 인공지능(AI) 연구원 초봉이 86만5000달러(약 11억3000만원)로 업계 최고 수준인 것으로 나타났습니다. AI 분야의 인재 부족 문제로 인해 최고급 스타트업과 빅테크 기업들이 9억~10억원에 달하는 초봉을 제시하는 상황이 벌어지고 있습니다.</p>\n",
    "\n",
    "        <p>급여 협상 서비스 기업 로라의 집계에 따르면 신규 박사급 AI 연구원을 채용한 600여개 기업 중 오픈AI와 앤트로픽이 각각 86만5000달러와 85만5000달러(약 11억2000만원)로 가장 높은 초봉을 제공했다고 합니다. 초봉에는 기본급과 보너스, 주식 등이 포함되어 있습니다.</p>\n",
    "\n",
    "        <p>인플렉션 AI는 82만5000달러(약 10억8000만원)로 3위, 이어서 테슬라, 아마존, 구글 브레인 등이 뒤를 잇는 것으로 발표되었으며, 전문 스타트업들이 빅테크 기업보다 더 치열한 인재 확보 경쟁을 벌이고 있는 것으로 파악되었습니다.</p>\n",
    "\n",
    "        <p>구글 리서치 같은 경우, 초기 제안과 최종 제안 사이의 협상폭이 77%로 상당히 높은 편으로 기록되었습니다. 한 연구원은 협상을 통해 초봉이 243% 증가한 것으로 나타났습니다. AI 기술에 대한 전 세계적인 수요가 공급을 웃돌아 연봉 수준이 높아지고 있습니다.</p>\n",
    "\n",
    "        <p>톨비 서베이의 설문조사 결과에 따르면, 컴퓨팅 연구 분야에서 수여된 박사 학위는 1691명에 불과하지만, 미국에서만 매년 수만 명의 컴퓨터 및 정보 연구원들이 필요한 상황입니다. 컴퓨터 비전, 로봇공학, NLP, 생물학, 신경과학 등에 AI를 적용하는 분야에서 수요가 가장 높다고 합니다.</p>\n",
    "\n",
    "        <p>최근 '챗GPT'의 등장으로 대형언어모델(LLM)에 대한 전문성이 인기 기술로 떠올랐습니다. AI 연구원들에게는 연구 능력 검증이 중요하며, 논문 출판 기록이 중요한 기준 중 하나로 여겨지고 있습니다. 업계 최고 수준의 연구원들이 지닌 논문은 최대 2000번의 인용과 'H-지수(H-index) 10'에 이르는 영향력을 나타내기도 합니다.</p>\n",
    "        </Example>\n",
    "        \"\"\"\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-1106-preview\",  # Specify the chat model here\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ]\n",
    "    )\n",
    "    reformatted_content = response.choices[0].message['content'].strip()\n",
    "    return reformatted_content\n",
    "\n",
    "reformated_content = reformat_content_for_blog(test_content[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태그 추가\n",
    "\n",
    "기사를 읽고 google seo에 맞는 태그 10가지를 받아옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformated_content = \"\"\"\n",
    "<p>오픈AI의 박사급 인공지능(AI) 연구원 초봉이 86만5000달러(약 11억3000만원)로 업계 최고 수준인 것으로 나타났습니다. AI 분야의 인재 부족 문제로 인해 최고급 스타트업과 빅테크 기업들이 9억~10억원에 달하는 초봉을 제시하는 상황이 벌어지고 있습니다.</p><br>\n",
    "\n",
    "<p>급여 협상 서비스 기업 로라의 집계에 따르면 신규 박사급 AI 연구원을 채용한 600여개 기업 중 오픈AI와 앤트로픽이 각각 86만5000달러와 85만5000달러(약 11억2000만원)로 가장 높은 초봉을 제공했다고 합니다. 초봉에는 기본급과 보너스, 주식 등이 포함되어 있습니다.</p><br>\n",
    "\n",
    "<p>인플렉션 AI는 82만5000달러(약 10억8000만원)로 3위, 이어서 테슬라, 아마존, 구글 브레인 등이 뒤를 잇는 것으로 발표되었으며, 전문 스타트업들이 빅테크 기업보다 더 치열한 인재 확보 경쟁을 벌이고 있는 것으로 파악되었습니다.</p><br>\n",
    "\n",
    "<p>구글 리서치 같은 경우, 초기 제안과 최종 제안 사이의 협상폭이 77%로 상당히 높은 편으로 기록되었습니다. 한 연구원은 협상을 통해 초봉이 243% 증가한 것으로 나타났습니다. AI 기술에 대한 전 세계적인 수요가 공급을 웃돌아 연봉 수준이 높아지고 있습니다.</p><br>\n",
    "\n",
    "<p>톨비 서베이의 설문조사 결과에 따르면, 컴퓨팅 연구 분야에서 수여된 박사 학위는 1691명에 불과하지만, 미국에서만 매년 수만 명의 컴퓨터 및 정보 연구원들이 필요한 상황입니다. 컴퓨터 비전, 로봇공학, NLP, 생물학, 신경과학 등에 AI를 적용하는 분야에서 수요가 가장 높다고 합니다.</p><br>\n",
    "\n",
    "<p>최근 '챗GPT'의 등장으로 대형언어모델(LLM)에 대한 전문성이 인기 기술로 떠올랐습니다. AI 연구원들에게는 연구 능력 검증이 중요하며, 논문 출판 기록이 중요한 기준 중 하나로 여겨지고 있습니다. 업계 최고 수준의 연구원들이 지닌 논문은 최대 2000번의 인용과 'H-지수(H-index) 10'에 이르는 영향력을 나타내기도 합니다.</p><br>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능, 초봉, 오픈AI, 연구원, 인력부족, 스타트업, 빅테크, 협상력, 박사학위, 대형언어모델\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Get_SEO_tag(content):\n",
    "    instruction = (\n",
    "        \"\"\"\n",
    "        <persona>\n",
    "        -You are Google SEO expert, I help with keyword selection, key keyword analysis, content planning, and blog post drafting.\n",
    "        </persona>\n",
    "\t    <Answer>\n",
    "        -Give all answers in Korean and create a maximum of 10 tags in total. I don't need more than that.\n",
    "        -Make tags no longer than two words.\n",
    "        -The answer is, don't add anything other than tags and commas.\n",
    "\t    </Answer>\n",
    "        <Example>\n",
    "        업스테이지, 콴다, 인공지능, 인공지능(AI), 솔라, SOLAR\n",
    "        </Example>\n",
    "        <Role>\n",
    "        -Provide information based on the instructions in <Answer>, <Example>.\n",
    "        </Role>\n",
    "        \"\"\"\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ]\n",
    "    )\n",
    "    tags = response.choices[0].message['content'].strip()\n",
    "    return tags\n",
    "\n",
    "get_tags = Get_SEO_tag(reformated_content)\n",
    "print(get_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언제든 삭제\n",
    "\n",
    "과금을 방지하고자 reformated_content 라는 변수를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import json\n",
    "\n",
    "client_id = \"68b7368ceec4522809c75ab1a00553ea\"\n",
    "secret_key = \"68b7368ceec4522809c75ab1a00553ea2c78d36135d7f26e02e6d843a5cf43e5755e276f\"\n",
    "access_token = \"5d692352dbcea2d43384a81c7f459de1_0590de3c23fcadafa068dfcb97ff68b4\"\n",
    "blogName = \"wzacorn\"\n",
    "\n",
    "tistory_url = 'https://www.tistory.com/apis/post/write' #url\n",
    "\n",
    "\n",
    "files = {'uploadedfile': open('thumbnail.png', 'rb')}\n",
    "params = {'access_token': access_token, 'blogName': blogName, 'targetUrl': blogName, 'output': 'json'}\n",
    "rd = requests.post('https://www.tistory.com/apis/post/attach', params=params, files=files)\n",
    "item = json.loads(rd.text)\n",
    "image_url = item['tistory']['url'] \n",
    "\n",
    "\n",
    "content = '<img src=\"' + image_url + '\">'\n",
    "content += reformated_content\n",
    "\n",
    "parameters = {\n",
    "    'access_token' : access_token,\n",
    "    'output' : '{output-type}',\n",
    "    'blogName' : blogName,\n",
    "    'title' : test_content[0],\n",
    "    'content' : content,\n",
    "    'visibility' : '3',\n",
    "    'category' : '1147172',\n",
    "    'tag' : get_tags,\n",
    "    'acceptComment' : '2'\n",
    "}\n",
    "\n",
    "requests.post(tistory_url, params=parameters)  #최종완료 Response가 200일시 완료임."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
